{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Automatic Differentiation with `torch.autograd`**\n",
        "\n",
        "\n",
        "When training neural networks, the most frequently used algorithm is\n",
        "**back propagation**. In this algorithm, parameters (model weights) are\n",
        "adjusted according to the **gradient** of the loss function with respect\n",
        "to the given parameter.\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation\n",
        "engine called `torch.autograd`. It supports automatic computation of\n",
        "gradient for any computational graph.\n",
        "\n",
        "Consider the simplest one-layer neural network, with input `x`,\n",
        "parameters `w` and `b`, and Binary Cross Entropy loss function for the cases below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Example - 01**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Manual Computation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UXPFYGopf4KV"
      },
      "outputs": [],
      "source": [
        "def dy_dx(x):\n",
        "    return 2*x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f256ernlf_ce",
        "outputId": "d9de562e-b841-4046-fbe2-f6f20b834f61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dy_dx(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using `autograd`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bm_ra2CG21GV"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nvMNlqM521TC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6_iSjcNw21cK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x**2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hsmrBTVv4r1A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.backward()\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz4fg9YR4s3V",
        "outputId": "19a16461-ab37-4bd9-a629-7752db77b79b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Example - 02**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Manual Computation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d_66YDf8gFwS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def dz_dx(x):\n",
        "    return 2 * x * math.cos(x**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPYLAIwgLSW",
        "outputId": "c3a1aa49-e75b-4ab5-b189-b9ee0eac52c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-7.661275842587077"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dz_dx(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using `autograd`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tYHUyJhBgO9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "17J3r8mZ8ebE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(16., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5YDYJEwJ8f7u"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.2879, grad_fn=<SinBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = torch.sin(y)\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "L_gisRUH-GNJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.2879, grad_fn=<SinBackward0>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.backward()\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6avVSli-nzp",
        "outputId": "73cf34ea-a242-436c-fd4a-2af9c7051a18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-7.6613)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Example - 03**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Computational graph**\n",
        "\n",
        "\n",
        "This code defines the following **computational graph**:\n",
        "\n",
        "![Computation Graph](https://pytorch.org/tutorials/_static/img/basics/comp-graph.png)\n",
        "\n",
        "In this network, `w` and `b` are **parameters**, which we need to\n",
        "optimize. Thus, we need to be able to compute the gradients of loss\n",
        "function with respect to those variables. In order to do that, we set\n",
        "the `requires_grad` property of those tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Manual Computation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IDA7vJD6TERe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CNcSnxKFVw6_"
      },
      "outputs": [],
      "source": [
        "# Binary Cross-Entropy Loss for scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8  # To prevent log(0)\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ysa6OOlAVzkI"
      },
      "outputs": [],
      "source": [
        "# Forward pass\n",
        "z = w * x + b  # Weighted sum (linear part)\n",
        "\n",
        "y_pred = torch.sigmoid(z)  # Predicted probability\n",
        "\n",
        "# Compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0U1dFI2aX4n",
        "outputId": "ec4aae8b-1f8e-4b44-a474-217c8641b07f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N31_2LUfV2cb"
      },
      "outputs": [],
      "source": [
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSP5rszqV5GG",
        "outputId": "20bba7d8-627b-4b55-ac65-3f8c52530bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ],
      "source": [
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using `autograd`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3xdGCYz8V-Rh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.7000)\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9NTVaauoa1CZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1., requires_grad=True)\n",
            "tensor(0., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvuriIW3a9OY",
        "outputId": "73f8d8a9-1c71-4445-e2bf-d2aec803b08f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = w*x + b\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEAvbpcbBlU",
        "outputId": "2f9fdce3-0cab-4575-b1f5-0431a80a72a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKaUqr18bIBd",
        "outputId": "af84b2a4-0724-4a94-ffc4-2fe23f43b0c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Computing Gradients**\n",
        "\n",
        "To optimize weights of parameters in the neural network, we need to\n",
        "compute the derivatives of our loss function with respect to parameters,\n",
        "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
        "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of `x` and\n",
        "`y`. To compute those derivatives, we call `loss.backward()`, and then\n",
        "retrieve the values from `w.grad` and `b.grad`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q891gtHabNoB"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O27utwfFbiw1",
        "outputId": "b7bbb560-234b-42ae-d031-99ee63f0472d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "By using autograd function (dw): 6.6917619705200195\n",
            "By using autograd function (db): 0.9987704753875732\n"
          ]
        }
      ],
      "source": [
        "print(f\"By using autograd function (dw): {w.grad}\")\n",
        "print(f\"By using autograd function (db): {b.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Using Vector Input Tensor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "X = [X_1, X_2, X_3]\n",
        "$$\n",
        "\n",
        "$$\n",
        "X = [1.0, 2.0, 3.0]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yuKGIgDThWq1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], requires_grad=True)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "Y = mean(x^2)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "Y = \\frac{(X_1)^2 + (X_2)^2 + (X_3)^2}{3}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "Y = fxn(X_1, X_2, X_3)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFDXqOw7ikIM",
        "outputId": "1837ea13-d764-4362-9378-aa7d0ec2e2ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = (x**2).mean()\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_1} = \\frac{2x_1}{3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_2} = \\frac{2x_2}{3}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_3} = \\frac{2x_3}{3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tWE_JM2xio9Q"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_1} = \\frac{2}{3} = 0.67\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_2} = \\frac{4}{3} = 1.3\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Y}{\\partial X_3} = 2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le70Drbkit_2",
        "outputId": "19563540-9d7f-41a8-a8dc-10c07ead466a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Clearing Gradients**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The key concept behind `autograd` in PyTorch is **gradient accumulation**. Each time you call `.backward()` on a tensor, the gradients for all tensors with `requires_grad=True` are added (accumulated) to their `.grad` attributes. This means if you run the backward pass multiple times without clearing the gradients, the values in `.grad` will keep increasing, reflecting the sum of all computed gradients.\n",
        "\n",
        "**Why does this happen?**  \n",
        "This behavior is useful when training neural networks using mini-batches. You might want to accumulate gradients over several batches before updating the model parameters.\n",
        "\n",
        "**How to manage gradient accumulation:**  \n",
        "- To avoid unwanted accumulation, always clear gradients before a new backward pass using `.zero_()` on the `.grad` attribute:\n",
        "    ```python\n",
        "    x.grad.zero_()\n",
        "    ```\n",
        "- Alternatively, use `optimizer.zero_grad()` when working with optimizers.\n",
        "\n",
        "**Summary:**  \n",
        "- `.backward()` accumulates gradients in `.grad`.\n",
        "- Always clear gradients before a new backward pass unless you intentionally want to accumulate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUAtKSFivZ1",
        "outputId": "1a36b60f-aa93-4305-a306-9ed74e67ae3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGml8iABkLUf",
        "outputId": "ada3587c-6180-4d63-8fcf-ca2eedd11414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1EkADF0enBdi"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60O_yfrInD_T",
        "outputId": "2c8fc324-171b-4ab9-d5ce-a42ae6fcbbdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQMMaFqoB4u",
        "outputId": "e232750b-2b55-4b34-c04f-1f2ae48018c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Disabling Gradient Tracking**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, we may need to perform computations without tracking gradients or calculating derivatives.\n",
        "\n",
        "**Scenarios Where Disabling Gradient Tracking is Useful**\n",
        "\n",
        "- **Model Inference:**  \n",
        "    When making predictions with a trained model, gradients are not needed.\n",
        "\n",
        "- **Model Evaluation:**  \n",
        "    During validation or testing phases, to save memory and computation.\n",
        "\n",
        "- **Feature Extraction:**  \n",
        "    When using a model to extract features from data without updating weights.\n",
        "\n",
        "- **Saving/Loading Model Outputs:**  \n",
        "    When storing intermediate results for later use.\n",
        "\n",
        "- **Visualizations:**  \n",
        "    When plotting or analyzing outputs that do not require gradients.\n",
        "\n",
        "- **Deployment:**  \n",
        "    In production environments where only forward passes are performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In such cases, PyTorch provides several ways to disable gradient tracking:\n",
        "\n",
        "- **Set `requires_grad` to `False`:**  \n",
        "    You can turn off gradient tracking for a tensor by setting its `requires_grad` attribute to `False` using `requires_grad_(False)`.\n",
        "\n",
        "- **Detach a tensor:**  \n",
        "    Use `.detach()` to create a new tensor that does not require gradients and is disconnected from the computation graph.\n",
        "\n",
        "- **Use `torch.no_grad()` context:**  \n",
        "    Wrap your code inside a `with torch.no_grad():` block to temporarily disable gradient tracking for all operations within the block.\n",
        "\n",
        "Disabling gradient tracking is useful for inference, evaluation, or any scenario where derivatives are not needed, as it reduces memory usage and speeds up computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyP-K6ooelo",
        "outputId": "04c330d8-d9ad-4be4-f104-cf09f4624c22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3tQ2LAq0n-",
        "outputId": "e8f60112-4d93-413a-83fa-5715a727b475"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x ** 2\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yyOCApZPr7zm"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQYxP_xr_1l",
        "outputId": "68fc0a41-87c8-47ec-9932-aa2d20ff2eda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using option 1 - `requires_grad_(False)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3J1W7BsLiK",
        "outputId": "fa8af314-ff87-4504-efae-0e9e088a94e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eqdqlwzsPR_",
        "outputId": "45b1302a-b661-4bde-8db5-853cb908dc9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "I6hTHHQZsQbP"
      },
      "outputs": [],
      "source": [
        "y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWh8lxdEsdRa",
        "outputId": "dac57dab-d690-416f-eb0a-5c118684ae92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DrW7KpiBsd6Z",
        "outputId": "24814268-474b-40c3-bb34-6555dfab0ceb"
      },
      "outputs": [],
      "source": [
        "# This will not work now:\n",
        "\n",
        "# y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using option 2 - `detach()`**\n",
        "\n",
        "The `.detach()` method in PyTorch creates a new tensor that shares the same data as the original tensor but does not require gradients and is disconnected from the computation graph. This is useful when you want to perform computations on a tensor without tracking gradients or affecting the autograd mechanism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Ct0xomsgw7",
        "outputId": "d679a65e-0d83-49c5-b8e2-ec112de84448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pslVLzLsmO0",
        "outputId": "3b6d6b97-9459-4e03-bb79-4b457a4782ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = x.detach()\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tXkAjBEcsp_C"
      },
      "outputs": [],
      "source": [
        "y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCm5rnLwsuMt",
        "outputId": "e60d73d8-2a39-4fc5-d943-99a27c3e3505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zK-PB97su18",
        "outputId": "60aedc8f-fb15-48f6-9586-016487c9e783"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y1 = z ** 2\n",
        "y1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_lQLYSmesxeX"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yvhtrhXMszVu",
        "outputId": "3cda73eb-f919-45fb-83f7-6b485057c070"
      },
      "outputs": [],
      "source": [
        "# This will not work now:\n",
        "\n",
        "# y1.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Using option 3 - `torch.no_grad()`**\n",
        "\n",
        "The `torch.no_grad()` context manager temporarily disables gradient tracking for all operations within its block. This is useful when you want to perform computations without building the computation graph or storing gradients, such as during model inference or evaluation. Any tensors created or modified inside the `with torch.no_grad():` block will not require gradients, even if their source tensors have `requires_grad=True`. This helps reduce memory usage and speeds up computations when gradients are not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrpCL8cbs0us",
        "outputId": "0db88907-9638-43c1-ac82-8acc2d671226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "n88-a1bxs569"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y = x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMsS-KPGs-x9",
        "outputId": "1b49a62d-2476-4c13-faad-12b7aa3c77ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GYN6bHVVs_OA"
      },
      "outputs": [],
      "source": [
        "# This will not work now:\n",
        "\n",
        "# y.backward()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
