{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9725f437"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "88e4d3e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af35c9f8"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "209b00fb",
        "outputId": "26105335-c436-4e20-dc47-d7a3a0e7fd22"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the capital of Germany?</td>\n",
              "      <td>Berlin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
              "      <td>Harper-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest planet in our solar system?</td>\n",
              "      <td>Jupiter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the boiling point of water in Celsius?</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          question      answer\n",
              "0                   What is the capital of France?       Paris\n",
              "1                  What is the capital of Germany?      Berlin\n",
              "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
              "3  What is the largest planet in our solar system?     Jupiter\n",
              "4   What is the boiling point of water in Celsius?         100"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('./100_Unique_QA_Dataset.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8c4e81"
      },
      "source": [
        "**Tokenize Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9e99e081"
      },
      "outputs": [],
      "source": [
        "# TOKENIZE\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('?', '')\n",
        "    text = text.replace(\"'\", \"\")\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c197140"
      },
      "source": [
        "**Tokenize Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c351655",
        "outputId": "4909f5d3-10f4-43a4-d093-e54bad44fd8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize(\"Who wrote 'To Kill a Mockingbird'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7da2fe35"
      },
      "source": [
        "**Initialize Vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "63bed1e2"
      },
      "outputs": [],
      "source": [
        "# VOCAB\n",
        "vocab = {'<UNK>': 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb544c1"
      },
      "source": [
        "**Build Vocabulary Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6fd58972"
      },
      "outputs": [],
      "source": [
        "def build_vocab(row):\n",
        "    tokenized_question = tokenize(row['question'])\n",
        "    tokenized_answer = tokenize(row['answer'])\n",
        "    \n",
        "    merged_tokens = tokenized_question + tokenized_answer\n",
        "    \n",
        "    print(merged_tokens)\n",
        "\n",
        "    for token in merged_tokens:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7bc706c"
      },
      "source": [
        "**Apply Build Vocabulary Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "_HcON4zGM9FK",
        "outputId": "5ca8e05e-b535-4bed-82c0-f49e5b730a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
            "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
            "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
            "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
            "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
            "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
            "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
            "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
            "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
            "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
            "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
            "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
            "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
            "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
            "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
            "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
            "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
            "['who', 'discovered', 'gravity', 'newton']\n",
            "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
            "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
            "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
            "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
            "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
            "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
            "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
            "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
            "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
            "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
            "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
            "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
            "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
            "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
            "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
            "['who', 'painted', 'starry', 'night', 'vangogh']\n",
            "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
            "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
            "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
            "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
            "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
            "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
            "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
            "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
            "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
            "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
            "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
            "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
            "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
            "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
            "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
            "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
            "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
            "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
            "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
            "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
            "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
            "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
            "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
            "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
            "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
            "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
            "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
            "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
            "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
            "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
            "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
            "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
            "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
            "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
            "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
            "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
            "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
            "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
            "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
            "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
            "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
            "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
            "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
            "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
            "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
            "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
            "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
            "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
            "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
            "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
            "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
            "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
            "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
            "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
            "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
            "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0     None\n",
              "1     None\n",
              "2     None\n",
              "3     None\n",
              "4     None\n",
              "      ... \n",
              "85    None\n",
              "86    None\n",
              "87    None\n",
              "88    None\n",
              "89    None\n",
              "Length: 90, dtype: object"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.apply(build_vocab, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51664d8e"
      },
      "source": [
        "**Vocabulary Size**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFO30aG6M_gu",
        "outputId": "cf650509-07ac-4a92-e78f-eb55c89af20e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b71bae57"
      },
      "source": [
        "**Accessing Vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " 'paris': 7,\n",
              " 'germany': 8,\n",
              " 'berlin': 9,\n",
              " 'who': 10,\n",
              " 'wrote': 11,\n",
              " 'to': 12,\n",
              " 'kill': 13,\n",
              " 'a': 14,\n",
              " 'mockingbird': 15,\n",
              " 'harper-lee': 16,\n",
              " 'largest': 17,\n",
              " 'planet': 18,\n",
              " 'in': 19,\n",
              " 'our': 20,\n",
              " 'solar': 21,\n",
              " 'system': 22,\n",
              " 'jupiter': 23,\n",
              " 'boiling': 24,\n",
              " 'point': 25,\n",
              " 'water': 26,\n",
              " 'celsius': 27,\n",
              " '100': 28,\n",
              " 'painted': 29,\n",
              " 'mona': 30,\n",
              " 'lisa': 31,\n",
              " 'leonardo-da-vinci': 32,\n",
              " 'square': 33,\n",
              " 'root': 34,\n",
              " '64': 35,\n",
              " '8': 36,\n",
              " 'chemical': 37,\n",
              " 'symbol': 38,\n",
              " 'for': 39,\n",
              " 'gold': 40,\n",
              " 'au': 41,\n",
              " 'which': 42,\n",
              " 'year': 43,\n",
              " 'did': 44,\n",
              " 'world': 45,\n",
              " 'war': 46,\n",
              " 'ii': 47,\n",
              " 'end': 48,\n",
              " '1945': 49,\n",
              " 'longest': 50,\n",
              " 'river': 51,\n",
              " 'nile': 52,\n",
              " 'japan': 53,\n",
              " 'tokyo': 54,\n",
              " 'developed': 55,\n",
              " 'theory': 56,\n",
              " 'relativity': 57,\n",
              " 'albert-einstein': 58,\n",
              " 'freezing': 59,\n",
              " 'fahrenheit': 60,\n",
              " '32': 61,\n",
              " 'known': 62,\n",
              " 'as': 63,\n",
              " 'red': 64,\n",
              " 'mars': 65,\n",
              " 'author': 66,\n",
              " '1984': 67,\n",
              " 'george-orwell': 68,\n",
              " 'currency': 69,\n",
              " 'united': 70,\n",
              " 'kingdom': 71,\n",
              " 'pound': 72,\n",
              " 'india': 73,\n",
              " 'delhi': 74,\n",
              " 'discovered': 75,\n",
              " 'gravity': 76,\n",
              " 'newton': 77,\n",
              " 'how': 78,\n",
              " 'many': 79,\n",
              " 'continents': 80,\n",
              " 'are': 81,\n",
              " 'there': 82,\n",
              " 'on': 83,\n",
              " 'earth': 84,\n",
              " '7': 85,\n",
              " 'gas': 86,\n",
              " 'do': 87,\n",
              " 'plants': 88,\n",
              " 'use': 89,\n",
              " 'photosynthesis': 90,\n",
              " 'co2': 91,\n",
              " 'smallest': 92,\n",
              " 'prime': 93,\n",
              " 'number': 94,\n",
              " '2': 95,\n",
              " 'invented': 96,\n",
              " 'telephone': 97,\n",
              " 'alexander-graham-bell': 98,\n",
              " 'australia': 99,\n",
              " 'canberra': 100,\n",
              " 'ocean': 101,\n",
              " 'pacific-ocean': 102,\n",
              " 'speed': 103,\n",
              " 'light': 104,\n",
              " 'vacuum': 105,\n",
              " '299,792,458m/s': 106,\n",
              " 'language': 107,\n",
              " 'spoken': 108,\n",
              " 'brazil': 109,\n",
              " 'portuguese': 110,\n",
              " 'penicillin': 111,\n",
              " 'alexander-fleming': 112,\n",
              " 'canada': 113,\n",
              " 'ottawa': 114,\n",
              " 'mammal': 115,\n",
              " 'whale': 116,\n",
              " 'element': 117,\n",
              " 'has': 118,\n",
              " 'atomic': 119,\n",
              " '1': 120,\n",
              " 'hydrogen': 121,\n",
              " 'tallest': 122,\n",
              " 'mountain': 123,\n",
              " 'everest': 124,\n",
              " 'city': 125,\n",
              " 'big': 126,\n",
              " 'apple': 127,\n",
              " 'newyork': 128,\n",
              " 'planets': 129,\n",
              " 'starry': 130,\n",
              " 'night': 131,\n",
              " 'vangogh': 132,\n",
              " 'formula': 133,\n",
              " 'h2o': 134,\n",
              " 'italy': 135,\n",
              " 'rome': 136,\n",
              " 'country': 137,\n",
              " 'famous': 138,\n",
              " 'sushi': 139,\n",
              " 'was': 140,\n",
              " 'first': 141,\n",
              " 'person': 142,\n",
              " 'step': 143,\n",
              " 'moon': 144,\n",
              " 'armstrong': 145,\n",
              " 'main': 146,\n",
              " 'ingredient': 147,\n",
              " 'guacamole': 148,\n",
              " 'avocado': 149,\n",
              " 'sides': 150,\n",
              " 'does': 151,\n",
              " 'hexagon': 152,\n",
              " 'have': 153,\n",
              " '6': 154,\n",
              " 'china': 155,\n",
              " 'yuan': 156,\n",
              " 'pride': 157,\n",
              " 'and': 158,\n",
              " 'prejudice': 159,\n",
              " 'jane-austen': 160,\n",
              " 'iron': 161,\n",
              " 'fe': 162,\n",
              " 'hardest': 163,\n",
              " 'natural': 164,\n",
              " 'substance': 165,\n",
              " 'diamond': 166,\n",
              " 'continent': 167,\n",
              " 'by': 168,\n",
              " 'area': 169,\n",
              " 'asia': 170,\n",
              " 'president': 171,\n",
              " 'states': 172,\n",
              " 'george-washington': 173,\n",
              " 'bird': 174,\n",
              " 'its': 175,\n",
              " 'ability': 176,\n",
              " 'mimic': 177,\n",
              " 'sounds': 178,\n",
              " 'parrot': 179,\n",
              " 'longest-running': 180,\n",
              " 'animated': 181,\n",
              " 'tv': 182,\n",
              " 'show': 183,\n",
              " 'simpsons': 184,\n",
              " 'vaticancity': 185,\n",
              " 'most': 186,\n",
              " 'moons': 187,\n",
              " 'saturn': 188,\n",
              " 'romeo': 189,\n",
              " 'juliet': 190,\n",
              " 'shakespeare': 191,\n",
              " 'earths': 192,\n",
              " 'atmosphere': 193,\n",
              " 'nitrogen': 194,\n",
              " 'bones': 195,\n",
              " 'adult': 196,\n",
              " 'human': 197,\n",
              " 'body': 198,\n",
              " '206': 199,\n",
              " 'metal': 200,\n",
              " 'liquid': 201,\n",
              " 'at': 202,\n",
              " 'room': 203,\n",
              " 'temperature': 204,\n",
              " 'mercury': 205,\n",
              " 'russia': 206,\n",
              " 'moscow': 207,\n",
              " 'electricity': 208,\n",
              " 'benjamin-franklin': 209,\n",
              " 'second-largest': 210,\n",
              " 'land': 211,\n",
              " 'color': 212,\n",
              " 'ripe': 213,\n",
              " 'banana': 214,\n",
              " 'yellow': 215,\n",
              " 'month': 216,\n",
              " '28': 217,\n",
              " 'days': 218,\n",
              " 'common': 219,\n",
              " 'february': 220,\n",
              " 'study': 221,\n",
              " 'living': 222,\n",
              " 'organisms': 223,\n",
              " 'called': 224,\n",
              " 'biology': 225,\n",
              " 'home': 226,\n",
              " 'great': 227,\n",
              " 'wall': 228,\n",
              " 'bees': 229,\n",
              " 'collect': 230,\n",
              " 'from': 231,\n",
              " 'flowers': 232,\n",
              " 'nectar': 233,\n",
              " 'opposite': 234,\n",
              " 'day': 235,\n",
              " 'south': 236,\n",
              " 'korea': 237,\n",
              " 'seoul': 238,\n",
              " 'bulb': 239,\n",
              " 'edison': 240,\n",
              " 'humans': 241,\n",
              " 'breathe': 242,\n",
              " 'survival': 243,\n",
              " 'oxygen': 244,\n",
              " '144': 245,\n",
              " '12': 246,\n",
              " 'pyramids': 247,\n",
              " 'giza': 248,\n",
              " 'egypt': 249,\n",
              " 'sea': 250,\n",
              " 'creature': 251,\n",
              " 'eight': 252,\n",
              " 'arms': 253,\n",
              " 'octopus': 254,\n",
              " 'holiday': 255,\n",
              " 'celebrated': 256,\n",
              " 'december': 257,\n",
              " '25': 258,\n",
              " 'christmas': 259,\n",
              " 'yen': 260,\n",
              " 'legs': 261,\n",
              " 'spider': 262,\n",
              " 'sport': 263,\n",
              " 'uses': 264,\n",
              " 'net,': 265,\n",
              " 'ball,': 266,\n",
              " 'hoop': 267,\n",
              " 'basketball': 268,\n",
              " 'kangaroos': 269,\n",
              " 'female': 270,\n",
              " 'minister': 271,\n",
              " 'uk': 272,\n",
              " 'margaretthatcher': 273,\n",
              " 'fastest': 274,\n",
              " 'animal': 275,\n",
              " 'cheetah': 276,\n",
              " 'periodic': 277,\n",
              " 'table': 278,\n",
              " 'spain': 279,\n",
              " 'madrid': 280,\n",
              " 'closest': 281,\n",
              " 'sun': 282,\n",
              " 'father': 283,\n",
              " 'computers': 284,\n",
              " 'charlesbabbage': 285,\n",
              " 'mexico': 286,\n",
              " 'mexicocity': 287,\n",
              " 'colors': 288,\n",
              " 'rainbow': 289,\n",
              " 'musical': 290,\n",
              " 'instrument': 291,\n",
              " 'black': 292,\n",
              " 'white': 293,\n",
              " 'keys': 294,\n",
              " 'piano': 295,\n",
              " 'americas': 296,\n",
              " '1492': 297,\n",
              " 'christophercolumbus': 298,\n",
              " 'disney': 299,\n",
              " 'character': 300,\n",
              " 'long': 301,\n",
              " 'nose': 302,\n",
              " 'grows': 303,\n",
              " 'it': 304,\n",
              " 'when': 305,\n",
              " 'lying': 306,\n",
              " 'pinocchio': 307,\n",
              " 'directed': 308,\n",
              " 'movie': 309,\n",
              " 'titanic': 310,\n",
              " 'jamescameron': 311,\n",
              " 'superhero': 312,\n",
              " 'also': 313,\n",
              " 'dark': 314,\n",
              " 'knight': 315,\n",
              " 'batman': 316,\n",
              " 'brasilia': 317,\n",
              " 'fruit': 318,\n",
              " 'king': 319,\n",
              " 'fruits': 320,\n",
              " 'mango': 321,\n",
              " 'eiffel': 322,\n",
              " 'tower': 323}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7d1bc0"
      },
      "source": [
        "**Convert Text to Indices Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tGzsYl6UNBsI"
      },
      "outputs": [],
      "source": [
        "# CONVERTING WORDS TO NUMERICAL INDICES\n",
        "def text_to_indices(text, vocab):\n",
        "\n",
        "    indexed_text = []\n",
        "    for token in tokenize(text):\n",
        "        if token in vocab:\n",
        "            indexed_text.append(vocab[token])\n",
        "        else:\n",
        "            indexed_text.append(vocab['<UNK>'])\n",
        "\n",
        "    return indexed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik-8lqU1NaRG",
        "outputId": "2c48c92d-2bf1-4db9-9ebb-b9c8183832be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 14, 4]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_to_indices(\"What is a capital?\", vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 14, 0]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_to_indices(\"What is a RNN?\", vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "k-haYG7WzjHj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "PElUlPYT0gqK"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, vocab):\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        numerical_question = text_to_indices(self.df.iloc[index]['question'], self.vocab)\n",
        "        numerical_answer = text_to_indices(self.df.iloc[index]['answer'], self.vocab)\n",
        "\n",
        "        return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "InSZ-ZIm1Y1O"
      },
      "outputs": [],
      "source": [
        "dataset = QADataset(df, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "BMVDt3h-1gMF"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642ec910"
      },
      "source": [
        "**Dataloader Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40dc2e29",
        "outputId": "5356e59b-9b48-4adb-a91a-0cc8d24b98fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([205])\n",
            "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([100])\n",
            "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([298])\n",
            "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([114])\n",
            "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([166])\n",
            "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([307])\n",
            "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([170])\n",
            "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([113])\n",
            "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([106])\n",
            "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([136])\n",
            "tensor([[ 10,  96,   3, 104, 239]]) tensor([240])\n",
            "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([254])\n",
            "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([116])\n",
            "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([154])\n",
            "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([285])\n",
            "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([280])\n",
            "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([49])\n",
            "tensor([[10, 55,  3, 56,  5, 57]]) tensor([58])\n",
            "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([295])\n",
            "tensor([[10, 29,  3, 30, 31]]) tensor([32])\n",
            "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([36])\n",
            "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([91])\n",
            "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([6])\n",
            "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([52])\n",
            "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([85])\n",
            "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([53])\n",
            "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([149])\n",
            "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([74])\n",
            "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([41])\n",
            "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([205])\n",
            "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([124])\n",
            "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([268])\n",
            "tensor([[ 10,  75, 208]]) tensor([209])\n",
            "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([317])\n",
            "tensor([[10, 96,  3, 97]]) tensor([98])\n",
            "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([199])\n",
            "tensor([[1, 2, 3, 4, 5, 8]]) tensor([9])\n",
            "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([36])\n",
            "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([225])\n",
            "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([99])\n",
            "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([28])\n",
            "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([85])\n",
            "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([233])\n",
            "tensor([[ 42, 101,   2,   3,  17]]) tensor([102])\n",
            "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([184])\n",
            "tensor([[1, 2, 3, 4, 5, 6]]) tensor([7])\n",
            "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([287])\n",
            "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([95])\n",
            "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([185])\n",
            "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([321])\n",
            "tensor([[10, 75, 76]]) tensor([77])\n",
            "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([249])\n",
            "tensor([[ 10,  11, 157, 158, 159]]) tensor([160])\n",
            "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([72])\n",
            "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([121])\n",
            "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([244])\n",
            "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([61])\n",
            "tensor([[10, 11, 12, 13, 14, 15]]) tensor([16])\n",
            "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([23])\n",
            "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([260])\n",
            "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([188])\n",
            "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([259])\n",
            "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([156])\n",
            "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([110])\n",
            "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([220])\n",
            "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([246])\n",
            "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([134])\n",
            "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([155])\n",
            "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([54])\n",
            "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([238])\n",
            "tensor([[ 10,  29, 130, 131]]) tensor([132])\n",
            "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([207])\n",
            "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([273])\n",
            "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([173])\n",
            "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([179])\n",
            "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([65])\n",
            "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([194])\n",
            "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([162])\n",
            "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([121])\n",
            "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([316])\n",
            "tensor([[10,  2,  3, 66,  5, 67]]) tensor([68])\n",
            "tensor([[ 10,  11, 189, 158, 190]]) tensor([191])\n",
            "tensor([[ 10,  75, 111]]) tensor([112])\n",
            "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([215])\n",
            "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([36])\n",
            "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([276])\n",
            "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([128])\n",
            "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([145])\n",
            "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([131])\n",
            "tensor([[ 10, 308,   3, 309, 310]]) tensor([311])\n"
          ]
        }
      ],
      "source": [
        "for question, answer in dataloader:\n",
        "    print(question, answer[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01c129be"
      },
      "source": [
        "**Simple RNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "SrJNCywq14Qv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "59cac880"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
        "        self.rnn = nn.RNN(50, 64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, question):\n",
        "        embedded_question = self.embedding(question)\n",
        "        hidden, final = self.rnn(embedded_question)\n",
        "        output = self.fc(final.squeeze(0))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1711f21c"
      },
      "source": [
        "**Model Architecture Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34d23c5",
        "outputId": "f07d3f5e-2610-440b-b256-bb8fcd6bbd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of a: torch.Size([1, 6])\n",
            "shape of b: torch.Size([1, 6, 50])\n",
            "shape of c: torch.Size([1, 6, 64])\n",
            "shape of d: torch.Size([1, 1, 64])\n",
            "shape of e: torch.Size([1, 324])\n"
          ]
        }
      ],
      "source": [
        "x = nn.Embedding(324, embedding_dim=50)\n",
        "y = nn.RNN(50, 64, batch_first=True)\n",
        "z = nn.Linear(64, 324)\n",
        "\n",
        "\n",
        "a = dataset[0][0].reshape(1,6)\n",
        "print(\"shape of a:\", a.shape)\n",
        "\n",
        "\n",
        "b = x(a)\n",
        "print(\"shape of b:\", b.shape)\n",
        "\n",
        "\n",
        "c, d = y(b)\n",
        "print(\"shape of c:\", c.shape)\n",
        "print(\"shape of d:\", d.shape)\n",
        "\n",
        "\n",
        "e = z(d.squeeze(0))\n",
        "print(\"shape of e:\", e.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5096d4f"
      },
      "source": [
        "**Learning Rate and Epochs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6072509c"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2af41be"
      },
      "source": [
        "**Initialize Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9a12b8e5"
      },
      "outputs": [],
      "source": [
        "model = SimpleRNN(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49b5a1dc"
      },
      "source": [
        "**Loss Function and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "59853a7e"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e373898d"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313c7e65",
        "outputId": "311d4379-b151-42fe-c887-af08bd83f374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Total Loss: 524.704813\n",
            "Epoch: 2, Total Loss: 454.730257\n",
            "Epoch: 3, Total Loss: 375.867527\n",
            "Epoch: 4, Total Loss: 311.789190\n",
            "Epoch: 5, Total Loss: 259.789129\n",
            "Epoch: 6, Total Loss: 211.704488\n",
            "Epoch: 7, Total Loss: 168.043764\n",
            "Epoch: 8, Total Loss: 130.885290\n",
            "Epoch: 9, Total Loss: 100.032030\n",
            "Epoch: 10, Total Loss: 76.734492\n",
            "Epoch: 11, Total Loss: 59.418766\n",
            "Epoch: 12, Total Loss: 46.638134\n",
            "Epoch: 13, Total Loss: 37.137787\n",
            "Epoch: 14, Total Loss: 30.338483\n",
            "Epoch: 15, Total Loss: 24.789986\n",
            "Epoch: 16, Total Loss: 20.861416\n",
            "Epoch: 17, Total Loss: 17.651503\n",
            "Epoch: 18, Total Loss: 15.154345\n",
            "Epoch: 19, Total Loss: 12.979251\n",
            "Epoch: 20, Total Loss: 11.490686\n",
            "Epoch: 21, Total Loss: 9.893620\n",
            "Epoch: 22, Total Loss: 8.727756\n",
            "Epoch: 23, Total Loss: 7.703395\n",
            "Epoch: 24, Total Loss: 6.832693\n",
            "Epoch: 25, Total Loss: 6.140806\n",
            "Epoch: 26, Total Loss: 5.571400\n",
            "Epoch: 27, Total Loss: 5.004740\n",
            "Epoch: 28, Total Loss: 4.554403\n",
            "Epoch: 29, Total Loss: 4.162269\n",
            "Epoch: 30, Total Loss: 3.808853\n",
            "Epoch: 31, Total Loss: 3.494954\n",
            "Epoch: 32, Total Loss: 3.217736\n",
            "Epoch: 33, Total Loss: 2.960514\n",
            "Epoch: 34, Total Loss: 2.745282\n",
            "Epoch: 35, Total Loss: 2.542238\n",
            "Epoch: 36, Total Loss: 2.361033\n",
            "Epoch: 37, Total Loss: 2.198174\n",
            "Epoch: 38, Total Loss: 2.052377\n",
            "Epoch: 39, Total Loss: 1.912862\n",
            "Epoch: 40, Total Loss: 1.784044\n",
            "Epoch: 41, Total Loss: 1.672653\n",
            "Epoch: 42, Total Loss: 1.568530\n",
            "Epoch: 43, Total Loss: 1.471138\n",
            "Epoch: 44, Total Loss: 1.383286\n",
            "Epoch: 45, Total Loss: 1.300048\n",
            "Epoch: 46, Total Loss: 1.221033\n",
            "Epoch: 47, Total Loss: 1.150657\n",
            "Epoch: 48, Total Loss: 1.084223\n",
            "Epoch: 49, Total Loss: 1.022262\n",
            "Epoch: 50, Total Loss: 0.965417\n"
          ]
        }
      ],
      "source": [
        "# TRAINING LOOP\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for question, answer in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # FORWARD PASS\n",
        "        output = model(question)\n",
        "\n",
        "        # LOSS -> OUTPUT SHAPE (1,324) - (1)\n",
        "        loss = criterion(output, answer[0])\n",
        "\n",
        "        # GRADIENTS\n",
        "        loss.backward()\n",
        "\n",
        "        # UPDATE\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Total Loss: {total_loss:4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58f47c8"
      },
      "source": [
        "**Predict Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "e8ced0a2"
      },
      "outputs": [],
      "source": [
        "def predict(model, question, threshold=0.5):\n",
        "\n",
        "    # CONVERT QUESTION TO NUMBERS\n",
        "    numerical_question = text_to_indices(question, vocab)\n",
        "\n",
        "    # TENSOR\n",
        "    question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
        "\n",
        "    # SEND TO MODEL\n",
        "    output = model(question_tensor)\n",
        "\n",
        "    # CONVERT LOGITS TO PROBS\n",
        "    probs = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "    # FIND INDEX OF MAX PROB\n",
        "    value, index = torch.max(probs, dim=1)\n",
        "\n",
        "\n",
        "    if value < threshold:\n",
        "        print(\"I don't know\")\n",
        "    else:\n",
        "        print(list(vocab.keys())[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20108991"
      },
      "source": [
        "**Predict Function Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42c24141",
        "outputId": "7751b939-cd6b-4ec4-823c-c7ed2e3534f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jupiter\n"
          ]
        }
      ],
      "source": [
        "predict(model, \"What is the largest planet in our solar system?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StNuxljVQXpr",
        "outputId": "99c05be1-5d89-4983-d741-5f68eb05107e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "paris\n"
          ]
        }
      ],
      "source": [
        "predict(model, 'What is the capital of France?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlud65HPQbi9",
        "outputId": "62480c14-d5b8-49a2-eb1b-0538a4703268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "paris\n",
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "# INTRODUCING SMALL BIT OF VARIATION TO THE QUESTION:\n",
        "predict(model, 'WHAT IS CAPITAL CITY OF FRANCE?')\n",
        "\n",
        "predict(model, 'What is the capital of a european country named France?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLmkO8BEQvu3",
        "outputId": "5d89a464-ec12-48e8-b2bb-615b270cbde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "# QUESTION WHICH MODEL HAS NOT SEEN EVEN ONCE:\n",
        "\n",
        "predict(model, 'Who is Avinash Yadav?')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
