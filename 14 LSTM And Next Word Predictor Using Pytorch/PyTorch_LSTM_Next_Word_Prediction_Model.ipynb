{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xi9dA1RzQdw"
      },
      "source": [
        "## **IMPORTING REQUIRED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ae7dunqczT8Q"
      },
      "outputs": [],
      "source": [
        "# IMPORTING NUMERICAL AND DATA PROCESSING LIBRARIES\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# IMPORTING PYTORCH LIBRARIES FOR DEEP LEARNING\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# IMPORTING NLTK FOR TEXT PROCESSING\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg2BiV0OzQd4"
      },
      "source": [
        "## **SAMPLE TEXT DOCUMENT FOR TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fLwm0Y_MzVuh"
      },
      "outputs": [],
      "source": [
        "# SAMPLE DOCUMENT CONTAINING FAQ DATA FOR TRAINING THE MODEL\n",
        "document = \"\"\"About the Program\n",
        "What is the course fee for  Data Science Mentorship Program (DSMP 2023)\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Python Fundamentals\n",
        "Python libraries for Data Science\n",
        "Data Analysis\n",
        "SQL for Data Science\n",
        "Maths for Machine Learning\n",
        "ML Algorithms\n",
        "Practical ML\n",
        "MLOPs\n",
        "Case studies\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390\n",
        "Will Deep Learning and NLP be a part of this program?\n",
        "No, NLP and Deep Learning both are not a part of this program's curriculum.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.\n",
        "Where can I find the class schedule?\n",
        "Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "What is the time duration of all the live sessions?\n",
        "Roughly, all the sessions last 2 hours.\n",
        "What is the language spoken by the instructor during the sessions?\n",
        "Hinglish\n",
        "How will I be informed about the upcoming class?\n",
        "You will get a mail from our side before every paid session once you become a paid user.\n",
        "Can I do this course if I am from a non-tech background?\n",
        "Yes, absolutely.\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "If I join/pay in the middle, will I be able to see all the past lectures?\n",
        "Yes, once you make the payment you will be able to see all the past content in your dashboard.\n",
        "Where do I have to submit the task?\n",
        "You don't have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.\n",
        "Will we do case studies in the program?\n",
        "Yes.\n",
        "Where can we contact you?\n",
        "You can mail us at nitish.campusx@gmail.com\n",
        "Payment/Registration related questions\n",
        "Where do we have to make our payments? Your YouTube channel or website?\n",
        "You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/\n",
        "Can we pay the entire amount of Rs 5600 all at once?\n",
        "Unfortunately no, the program follows a monthly subscription model.\n",
        "What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb\n",
        "15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don't have to wait for a month to end.\n",
        "What if I don't like the course after making the payment. What is the refund policy?\n",
        "You get a 7 days refund period from the day you have made the payment.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmail.com\n",
        "Post registration queries\n",
        "Till when can I view the paid videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.\n",
        "But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.\n",
        "Why lifetime validity is not provided?\n",
        "Because of the low course fee.\n",
        "Where can I reach out in case of a doubt after the session?\n",
        "You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session\n",
        "If I join the program late, can I still ask past week doubts?\n",
        "Yes, just select past week doubt in the doubt clearance google form.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmai.com\n",
        "Certificate and Placement Assistance related queries\n",
        "What is the criteria to get the certificate?\n",
        "There are 2 criterias:\n",
        "You have to pay the entire fee of Rs 5600\n",
        "You have to attempt all the course assessments.\n",
        "I am joining late. How can I pay payment of the earlier months?\n",
        "You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.\n",
        "I have read that Placement assistance is a part of this program. What comes under Placement assistance?\n",
        "This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance\n",
        "Portfolio Building sessions\n",
        "Soft skill sessions\n",
        "Sessions with industry mentors\n",
        "Discussion on Job hunting strategies\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nA8iXhkzQd7"
      },
      "source": [
        "## **DOWNLOADING NLTK RESOURCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMU_RwfbzXt4",
        "outputId": "63616862-b63c-4290-cf4d-d0c097852cdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\avina\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\avina\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DOWNLOADING NECESSARY NLTK DATA FOR TOKENIZATION\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjnZiB55zQd-"
      },
      "source": [
        "## **TEXT TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t28bgAcszaHl"
      },
      "outputs": [],
      "source": [
        "# TOKENIZE THE DOCUMENT INTO INDIVIDUAL WORDS AND CONVERT TO LOWERCASE\n",
        "tokens = word_tokenize(document.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6RuMIKpzQeA"
      },
      "source": [
        "## **VOCABULARY BUILDING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G30GxEjgzcfY",
        "outputId": "02408bb7-8960-40df-e5a5-6ee29bc8967f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<unk>': 0,\n",
              " 'about': 1,\n",
              " 'the': 2,\n",
              " 'program': 3,\n",
              " 'what': 4,\n",
              " 'is': 5,\n",
              " 'course': 6,\n",
              " 'fee': 7,\n",
              " 'for': 8,\n",
              " 'data': 9,\n",
              " 'science': 10,\n",
              " 'mentorship': 11,\n",
              " '(': 12,\n",
              " 'dsmp': 13,\n",
              " '2023': 14,\n",
              " ')': 15,\n",
              " 'follows': 16,\n",
              " 'a': 17,\n",
              " 'monthly': 18,\n",
              " 'subscription': 19,\n",
              " 'model': 20,\n",
              " 'where': 21,\n",
              " 'you': 22,\n",
              " 'have': 23,\n",
              " 'to': 24,\n",
              " 'make': 25,\n",
              " 'payments': 26,\n",
              " 'of': 27,\n",
              " 'rs': 28,\n",
              " '799/month': 29,\n",
              " '.': 30,\n",
              " 'total': 31,\n",
              " 'duration': 32,\n",
              " '?': 33,\n",
              " '7': 34,\n",
              " 'months': 35,\n",
              " 'so': 36,\n",
              " 'becomes': 37,\n",
              " '799': 38,\n",
              " '*': 39,\n",
              " '=': 40,\n",
              " '5600': 41,\n",
              " 'approx': 42,\n",
              " 'syllabus': 43,\n",
              " 'we': 44,\n",
              " 'will': 45,\n",
              " 'be': 46,\n",
              " 'covering': 47,\n",
              " 'following': 48,\n",
              " 'modules': 49,\n",
              " ':': 50,\n",
              " 'python': 51,\n",
              " 'fundamentals': 52,\n",
              " 'libraries': 53,\n",
              " 'analysis': 54,\n",
              " 'sql': 55,\n",
              " 'maths': 56,\n",
              " 'machine': 57,\n",
              " 'learning': 58,\n",
              " 'ml': 59,\n",
              " 'algorithms': 60,\n",
              " 'practical': 61,\n",
              " 'mlops': 62,\n",
              " 'case': 63,\n",
              " 'studies': 64,\n",
              " 'can': 65,\n",
              " 'check': 66,\n",
              " 'detailed': 67,\n",
              " 'here': 68,\n",
              " '-': 69,\n",
              " 'https': 70,\n",
              " '//learnwith.campusx.in/courses/campusx-data-science-mentorship-program-637339afe4b0615a1bbed390': 71,\n",
              " 'deep': 72,\n",
              " 'and': 73,\n",
              " 'nlp': 74,\n",
              " 'part': 75,\n",
              " 'this': 76,\n",
              " 'no': 77,\n",
              " ',': 78,\n",
              " 'both': 79,\n",
              " 'are': 80,\n",
              " 'not': 81,\n",
              " \"'s\": 82,\n",
              " 'curriculum': 83,\n",
              " 'if': 84,\n",
              " 'i': 85,\n",
              " 'miss': 86,\n",
              " 'live': 87,\n",
              " 'session': 88,\n",
              " 'get': 89,\n",
              " 'recording': 90,\n",
              " 'yes': 91,\n",
              " 'all': 92,\n",
              " 'our': 93,\n",
              " 'sessions': 94,\n",
              " 'recorded': 95,\n",
              " 'even': 96,\n",
              " 'go': 97,\n",
              " 'back': 98,\n",
              " 'watch': 99,\n",
              " 'find': 100,\n",
              " 'class': 101,\n",
              " 'schedule': 102,\n",
              " 'checkout': 103,\n",
              " 'google': 104,\n",
              " 'sheet': 105,\n",
              " 'see': 106,\n",
              " 'month': 107,\n",
              " 'by': 108,\n",
              " 'time': 109,\n",
              " 'table': 110,\n",
              " '//docs.google.com/spreadsheets/d/16ootax_a6oraecg4emgexhqqpv3noqpyku7rj6arozk/edit': 111,\n",
              " 'usp=sharing': 112,\n",
              " 'roughly': 113,\n",
              " 'last': 114,\n",
              " '2': 115,\n",
              " 'hours': 116,\n",
              " 'language': 117,\n",
              " 'spoken': 118,\n",
              " 'instructor': 119,\n",
              " 'during': 120,\n",
              " 'hinglish': 121,\n",
              " 'how': 122,\n",
              " 'informed': 123,\n",
              " 'upcoming': 124,\n",
              " 'mail': 125,\n",
              " 'from': 126,\n",
              " 'side': 127,\n",
              " 'before': 128,\n",
              " 'every': 129,\n",
              " 'paid': 130,\n",
              " 'once': 131,\n",
              " 'become': 132,\n",
              " 'user': 133,\n",
              " 'do': 134,\n",
              " 'am': 135,\n",
              " 'non-tech': 136,\n",
              " 'background': 137,\n",
              " 'absolutely': 138,\n",
              " 'late': 139,\n",
              " 'join': 140,\n",
              " 'in': 141,\n",
              " 'middle': 142,\n",
              " 'anytime': 143,\n",
              " 'join/pay': 144,\n",
              " 'able': 145,\n",
              " 'past': 146,\n",
              " 'lectures': 147,\n",
              " 'payment': 148,\n",
              " 'content': 149,\n",
              " 'your': 150,\n",
              " 'dashboard': 151,\n",
              " 'submit': 152,\n",
              " 'task': 153,\n",
              " \"n't\": 154,\n",
              " 'provide': 155,\n",
              " 'with': 156,\n",
              " 'solutions': 157,\n",
              " 'self': 158,\n",
              " 'evaluate': 159,\n",
              " 'yourself': 160,\n",
              " 'contact': 161,\n",
              " 'us': 162,\n",
              " 'at': 163,\n",
              " 'nitish.campusx': 164,\n",
              " '@': 165,\n",
              " 'gmail.com': 166,\n",
              " 'payment/registration': 167,\n",
              " 'related': 168,\n",
              " 'questions': 169,\n",
              " 'youtube': 170,\n",
              " 'channel': 171,\n",
              " 'or': 172,\n",
              " 'website': 173,\n",
              " 'on': 174,\n",
              " 'link': 175,\n",
              " '//learnwith.campusx.in/': 176,\n",
              " 'pay': 177,\n",
              " 'entire': 178,\n",
              " 'amount': 179,\n",
              " 'unfortunately': 180,\n",
              " 'validity': 181,\n",
              " 'suppose': 182,\n",
              " '15th': 183,\n",
              " 'jan': 184,\n",
              " 'then': 185,\n",
              " 'again': 186,\n",
              " '1st': 187,\n",
              " 'feb': 188,\n",
              " 'feb.': 189,\n",
              " 'period': 190,\n",
              " '30': 191,\n",
              " 'days': 192,\n",
              " 'day': 193,\n",
              " 'essentially': 194,\n",
              " 'wait': 195,\n",
              " 'end': 196,\n",
              " 'like': 197,\n",
              " 'after': 198,\n",
              " 'making': 199,\n",
              " 'refund': 200,\n",
              " 'policy': 201,\n",
              " 'made': 202,\n",
              " 'living': 203,\n",
              " 'outside': 204,\n",
              " 'india': 205,\n",
              " 'should': 206,\n",
              " 'sending': 207,\n",
              " 'post': 208,\n",
              " 'registration': 209,\n",
              " 'queries': 210,\n",
              " 'till': 211,\n",
              " 'when': 212,\n",
              " 'view': 213,\n",
              " 'videos': 214,\n",
              " 'one': 215,\n",
              " 'tricky': 216,\n",
              " 'read': 217,\n",
              " 'carefully': 218,\n",
              " 'valid': 219,\n",
              " 'purchased': 220,\n",
              " '21st': 221,\n",
              " '20th': 222,\n",
              " 'but': 223,\n",
              " 'purchase': 224,\n",
              " 'over': 225,\n",
              " 'installments': 226,\n",
              " 'aug': 227,\n",
              " '2024.': 228,\n",
              " 'why': 229,\n",
              " 'lifetime': 230,\n",
              " 'provided': 231,\n",
              " 'because': 232,\n",
              " 'low': 233,\n",
              " 'reach': 234,\n",
              " 'out': 235,\n",
              " 'doubt': 236,\n",
              " 'fill': 237,\n",
              " 'form': 238,\n",
              " 'team': 239,\n",
              " '1': 240,\n",
              " 'clearance': 241,\n",
              " 'still': 242,\n",
              " 'ask': 243,\n",
              " 'week': 244,\n",
              " 'doubts': 245,\n",
              " 'just': 246,\n",
              " 'select': 247,\n",
              " 'gmai.com': 248,\n",
              " 'certificate': 249,\n",
              " 'placement': 250,\n",
              " 'assistance': 251,\n",
              " 'criteria': 252,\n",
              " 'there': 253,\n",
              " 'criterias': 254,\n",
              " 'attempt': 255,\n",
              " 'assessments': 256,\n",
              " 'joining': 257,\n",
              " 'earlier': 258,\n",
              " 'current': 259,\n",
              " 'that': 260,\n",
              " 'comes': 261,\n",
              " 'under': 262,\n",
              " 'clarify': 263,\n",
              " 'does': 264,\n",
              " 'mean': 265,\n",
              " 'guarantee': 266,\n",
              " 'dont': 267,\n",
              " 'any': 268,\n",
              " 'jobs': 269,\n",
              " 'matter': 270,\n",
              " 'interview': 271,\n",
              " 'calls': 272,\n",
              " 'planning': 273,\n",
              " 'placements': 274,\n",
              " 'afraid': 275,\n",
              " 'disappointed': 276,\n",
              " 'portfolio': 277,\n",
              " 'building': 278,\n",
              " 'soft': 279,\n",
              " 'skill': 280,\n",
              " 'industry': 281,\n",
              " 'mentors': 282,\n",
              " 'discussion': 283,\n",
              " 'job': 284,\n",
              " 'hunting': 285,\n",
              " 'strategies': 286}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# BUILD VOCABULARY DICTIONARY WITH UNKNOWN TOKEN\n",
        "vocab = {'<unk>':0}\n",
        "\n",
        "# ADD EACH UNIQUE TOKEN TO VOCABULARY WITH UNIQUE INDEX\n",
        "for token in Counter(tokens).keys():\n",
        "    if token not in vocab:\n",
        "        vocab[token] = len(vocab)\n",
        "\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mufZt0zQeD"
      },
      "source": [
        "## **VOCABULARY SIZE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOOEZ94P0dQ1",
        "outputId": "290a7822-1365-4b70-ed66-b85a827325a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "287"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY THE TOTAL SIZE OF VOCABULARY\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExwz9O5zQeE"
      },
      "source": [
        "## **SENTENCE SPLITTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RefNavJe1Cva"
      },
      "outputs": [],
      "source": [
        "# SPLIT DOCUMENT INTO INDIVIDUAL SENTENCES BASED ON NEWLINES\n",
        "input_sentences = document.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-m0mZRzzQeF"
      },
      "source": [
        "## **TEXT TO INDICES CONVERSION FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x52A3E1K1zjn"
      },
      "outputs": [],
      "source": [
        "# FUNCTION TO CONVERT TEXT TOKENS TO NUMERICAL INDICES\n",
        "def text_to_indices(sentence, vocab):\n",
        "\n",
        "    # INITIALIZE EMPTY LIST FOR NUMERICAL REPRESENTATION\n",
        "    numerical_sentence = []\n",
        "\n",
        "    # CONVERT EACH TOKEN TO ITS VOCABULARY INDEX\n",
        "    for token in sentence:\n",
        "        if token in vocab:\n",
        "            numerical_sentence.append(vocab[token])\n",
        "        else:\n",
        "            # USE UNKNOWN TOKEN INDEX IF TOKEN NOT IN VOCABULARY\n",
        "            numerical_sentence.append(vocab['<unk>'])\n",
        "\n",
        "    return numerical_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RnOzGxSzQeG"
      },
      "source": [
        "## **CONVERTING SENTENCES TO NUMERICAL FORMAT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eu66Zo3e1Wh9"
      },
      "outputs": [],
      "source": [
        "# CONVERT ALL INPUT SENTENCES TO NUMERICAL REPRESENTATION\n",
        "input_numerical_sentences = []\n",
        "\n",
        "# PROCESS EACH SENTENCE AND CONVERT TO INDICES\n",
        "for sentence in input_sentences:\n",
        "    input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXhok7QzQeG"
      },
      "source": [
        "## **NUMBER OF PROCESSED SENTENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJesAQC1et3",
        "outputId": "35bdc2dd-2c13-404d-db2d-480043967ead"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY TOTAL NUMBER OF NUMERICAL SENTENCES\n",
        "len(input_numerical_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUFGEi1qzQeH"
      },
      "source": [
        "## **CREATING TRAINING SEQUENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "80rIx4aq6ele"
      },
      "outputs": [],
      "source": [
        "# CREATE TRAINING SEQUENCES FOR NEXT WORD PREDICTION\n",
        "training_sequence = []\n",
        "\n",
        "# GENERATE ALL POSSIBLE SUBSEQUENCES FROM EACH SENTENCE\n",
        "for sentence in input_numerical_sentences:\n",
        "    for i in range(1, len(sentence)):\n",
        "        training_sequence.append(sentence[:i+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTQb7f_4zQeI"
      },
      "source": [
        "## **TOTAL TRAINING SEQUENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_aGJ0fy7swk",
        "outputId": "0b284fdb-39ed-4053-a0b1-ce26c2f95f29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "938"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY TOTAL NUMBER OF TRAINING SEQUENCES GENERATED\n",
        "len(training_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU7cab19zQeI"
      },
      "source": [
        "## **SAMPLE TRAINING SEQUENCES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrFzZ4DD8Anu",
        "outputId": "189cfb32-ae6a-436f-dc2b-ddfd8c85937c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 2], [1, 2, 3], [4, 5], [4, 5, 2], [4, 5, 2, 6]]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY FIRST 5 TRAINING SEQUENCES AS EXAMPLES\n",
        "training_sequence[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8yBb3nGzQeJ"
      },
      "source": [
        "## **FINDING MAXIMUM SEQUENCE LENGTH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Z_fiVZ8GRo",
        "outputId": "05f9c9be-6fac-4da4-fc2d-cbbc07b2aca7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# CALCULATE THE MAXIMUM LENGTH OF ALL TRAINING SEQUENCES\n",
        "len_list = []\n",
        "\n",
        "# GET LENGTH OF EACH SEQUENCE\n",
        "for sequence in training_sequence:\n",
        "    len_list.append(len(sequence))\n",
        "\n",
        "# FIND MAXIMUM LENGTH FOR PADDING\n",
        "max(len_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbyFzsYhzQeK"
      },
      "source": [
        "## **SAMPLE TRAINING SEQUENCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bIcIRd088EN",
        "outputId": "89657211-b46d-4ced-cb46-655ce8d90f6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY THE FIRST TRAINING SEQUENCE\n",
        "training_sequence[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFtWRhnRzQeK"
      },
      "source": [
        "## **PADDING SEQUENCES TO UNIFORM LENGTH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dtPg5uRN9Cc7"
      },
      "outputs": [],
      "source": [
        "# PAD ALL SEQUENCES TO THE SAME LENGTH WITH ZEROS\n",
        "padded_training_sequence = []\n",
        "\n",
        "# ADD ZERO PADDING TO THE LEFT OF EACH SEQUENCE\n",
        "for sequence in training_sequence:\n",
        "    padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tOGRwZazQeK"
      },
      "source": [
        "## **VERIFYING PADDED SEQUENCE LENGTH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqZssF989X-4",
        "outputId": "b68aa7df-9465-4f87-cb46-9f1ac26e8fa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# VERIFY THAT ALL SEQUENCES NOW HAVE THE SAME LENGTH\n",
        "len(padded_training_sequence[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QZJLp34zQeL"
      },
      "source": [
        "## **CONVERTING TO PYTORCH TENSOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0_wVpepb9iE4"
      },
      "outputs": [],
      "source": [
        "# CONVERT PADDED SEQUENCES TO PYTORCH TENSOR\n",
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq1fSx4izQeM"
      },
      "source": [
        "## **DISPLAYING TENSOR STRUCTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([938, 62])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_training_sequence.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogKvdXa79yxV",
        "outputId": "7507c759-645c-4c66-e5d7-5722397e6d3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   1,   2,   3],\n",
              "        [  0,   0,   0,  ...,   0,   4,   5],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ..., 283, 174, 284],\n",
              "        [  0,   0,   0,  ..., 174, 284, 285],\n",
              "        [  0,   0,   0,  ..., 284, 285, 286]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY THE PADDED TRAINING SEQUENCE TENSOR\n",
        "padded_training_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407iikixzQeN"
      },
      "source": [
        "## **PREPARING INPUT-OUTPUT PAIRS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Tz8fwCok90m0"
      },
      "outputs": [],
      "source": [
        "# SPLIT SEQUENCES INTO INPUT (X) AND TARGET (Y) FOR TRAINING\n",
        "X = padded_training_sequence[:, :-1]  # ALL WORDS EXCEPT THE LAST\n",
        "y = padded_training_sequence[:,-1]    # ONLY THE LAST WORD AS TARGET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sTTihVwzQeO"
      },
      "source": [
        "## **INPUT FEATURES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ed_PLHJ-Dgv",
        "outputId": "64eebaf5-a2e5-40a6-a70c-c99002bb5cb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   0,   1],\n",
              "        [  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   0,   0,   4],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ...,   0, 283, 174],\n",
              "        [  0,   0,   0,  ..., 283, 174, 284],\n",
              "        [  0,   0,   0,  ..., 174, 284, 285]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY INPUT FEATURES (SEQUENCES WITHOUT LAST WORD)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZHgpR9izQeO"
      },
      "source": [
        "## **TARGET LABELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eReVrcX9-EUU",
        "outputId": "fe4b1574-ece0-4776-d8eb-f891a31aacda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  2,   3,   5,   2,   6,   7,   8,   9,  10,  11,   3,  12,  13,  14,\n",
              "         15,   6,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  18,  26,\n",
              "         27,  28,  29,  30,   5,   2,  31,  32,  27,   2,   6,  33,  31,  32,\n",
              "         27,   2,   6,   5,  34,  35,  30,  36,   2,  31,   6,   7,  37,  38,\n",
              "         39,  34,  40,  28,  41,  12,  42,  30,  15,   5,   2,  43,  27,   2,\n",
              "         11,   3,  33,  45,  46,  47,   2,  48,  49,  50,  52,  53,   8,   9,\n",
              "         10,  54,   8,   9,  10,   8,  57,  58,  60,  59,  64,  65,  66,   2,\n",
              "         67,  43,  68,  69,  70,  50,  71,  72,  58,  73,  74,  46,  17,  75,\n",
              "         27,  76,   3,  33,  78,  74,  73,  72,  58,  79,  80,  81,  17,  75,\n",
              "         27,  76,   3,  82,  83,  30,  84,  85,  86,  17,  87,  88,  33,  45,\n",
              "         85,  89,  17,  90,  27,   2,  88,  33,  92,  93,  94,  80,  95,  78,\n",
              "         36,  96,  84,  22,  86,  17,  88,  22,  65,  97,  98,  73,  99,   2,\n",
              "         90,  30,  65,  85, 100,   2, 101, 102,  33,  76, 104, 105,  24, 106,\n",
              "        107, 108, 107, 109, 110,  27,   2,   6,  69,  70,  50, 111,  33, 112,\n",
              "         30,   5,   2, 109,  32,  27,  92,   2,  87,  94,  33,  78,  92,   2,\n",
              "         94, 114, 115, 116,  30,   5,   2, 117, 118, 108,   2, 119, 120,   2,\n",
              "         94,  33,  45,  85,  46, 123,   1,   2, 124, 101,  33,  45,  89,  17,\n",
              "        125, 126,  93, 127, 128, 129, 130,  88, 131,  22, 132,  17, 130, 133,\n",
              "         30,  85, 134,  76,   6,  84,  85, 135, 126,  17, 136, 137,  33,  78,\n",
              "        138,  30, 135, 139,  78,  65,  85, 140,   2,   3, 141,   2, 142,  33,\n",
              "         78,  22,  65, 140,   2,   3, 143,  30,  85, 144, 141,   2, 142,  78,\n",
              "         45,  85,  46, 145,  24, 106,  92,   2, 146, 147,  33,  78, 131,  22,\n",
              "         25,   2, 148,  22,  45,  46, 145,  24, 106,  92,   2, 146, 149, 141,\n",
              "        150, 151,  30, 134,  85,  23,  24, 152,   2, 153,  33, 134, 154,  23,\n",
              "         24, 152,   2, 153,  30,  44,  45, 155,  22, 156,   2, 157,  78,  22,\n",
              "         23,  24, 158, 159,   2, 153, 160,  30,  44, 134,  63,  64, 141,   2,\n",
              "          3,  33,  30,  65,  44, 161,  22,  33,  65, 125, 162, 163, 164, 165,\n",
              "        166, 168, 169, 134,  44,  23,  24,  25,  93,  26,  33, 150, 170, 171,\n",
              "        172, 173,  33,  23,  24,  25,  92, 150,  18,  26, 174,  93, 173,  30,\n",
              "         68,   5,   2, 175,   8,  93, 173,  69,  70,  50, 176,  44, 177,   2,\n",
              "        178, 179,  27,  28,  41,  92, 163, 131,  33,  77,  78,   2,   3,  16,\n",
              "         17,  18,  19,  20,  30,   5,   2, 181,  27,  18,  19,  33, 182,  84,\n",
              "         85, 177, 174, 183, 184,  78, 185, 134,  85,  23,  24, 177, 186, 174,\n",
              "        187, 188, 172, 183, 188, 189,   2, 181, 190,   5, 191, 192, 126,   2,\n",
              "        193,  22,  25,   2, 148,  30,  36, 194,  22,  65, 140, 143,  22, 134,\n",
              "        154,  23,  24, 195,   8,  17, 107,  24, 196,  30,  84,  85, 134, 154,\n",
              "        197,   2,   6, 198, 199,   2, 148,  30,   4,   5,   2, 200, 201,  33,\n",
              "         89,  17,  34, 192, 200, 190, 126,   2, 193,  22,  23, 202,   2, 148,\n",
              "         30, 135, 203, 204, 205,  73,  85, 135,  81, 145,  24,  25,   2, 148,\n",
              "        174,   2, 173,  78,   4, 206,  85, 134,  33,  23,  24, 161, 162, 108,\n",
              "        207,  17, 125, 163, 164, 165, 166, 209, 210, 212,  65,  85, 213,   2,\n",
              "        130, 214, 174,   2, 173,  33, 215,   5, 216,  78,  36, 217, 218,  30,\n",
              "         22,  65,  99,   2, 214, 211, 150,  19,   5, 219,  30, 182,  22,  23,\n",
              "        220,  19, 174, 221, 184,  78,  22,  45,  46, 145,  24,  99,  92,   2,\n",
              "        146, 130,  94, 141,   2, 190,  27, 221, 184,  24, 222, 189, 223, 198,\n",
              "        221, 188,  22,  45,  23,  24, 224,   2,  19, 186,  30, 131,   2,   6,\n",
              "          5, 225,  73,  22,  23, 130, 162,  28,  41,  12, 172,  34, 226,  27,\n",
              "         28,  38,  15,  22,  45,  46, 145,  24,  99,   2, 130,  94, 211, 227,\n",
              "          0,  30, 230, 181,   5,  81, 231,  33,  27,   2, 233,   6,   7,  30,\n",
              "         65,  85, 234, 235, 141,  63,  27,  17, 236, 198,   2,  88,  33,  45,\n",
              "         23,  24, 237,  17, 104, 238, 231, 141, 150, 151,  73,  93, 239,  45,\n",
              "        161,  22,   8,  17, 240, 174, 240, 236, 241,  88,  85, 140,   2,   3,\n",
              "        139,  78,  65,  85, 242, 243, 146, 244, 245,  33,  78, 246, 247, 146,\n",
              "        244, 236, 141,   2, 236, 241, 104, 238,  30, 135, 203, 204, 205,  73,\n",
              "         85, 135,  81, 145,  24,  25,   2, 148, 174,   2, 173,  78,   4, 206,\n",
              "         85, 134,  33,  23,  24, 161, 162, 108, 207,  17, 125, 163, 164, 165,\n",
              "        248,  73, 250, 251, 168, 210,   5,   2, 252,  24,  89,   2, 249,  33,\n",
              "         80, 115, 254,  50,  23,  24, 177,   2, 178,   7,  27,  28,  41,  23,\n",
              "         24, 255,  92,   2,   6, 256,  30, 135, 257, 139,  30, 122,  65,  85,\n",
              "        177, 148,  27,   2, 258,  35,  33,  45,  89,  17, 175,  24, 177,   7,\n",
              "         27, 258,  35, 141, 150, 151, 131,  22, 177,   8,   2, 259, 107,  30,\n",
              "         23, 217, 260, 250, 251,   5,  17,  75,  27,  76,   3,  30,   4, 261,\n",
              "        262, 250, 251,  33,   5,  24, 263, 260, 250, 251, 264,  81, 265, 250,\n",
              "        266,  30,  36,  44, 267, 266,  22, 268, 269, 172,   8, 260, 270,  96,\n",
              "        271, 272,  30,  36,  84,  22,  80, 273,  24, 140,  76,   6, 246,   8,\n",
              "        274,  78,  85, 135, 275,  22,  45,  46, 276,  30,  68,   5,   4, 261,\n",
              "        262, 250, 251, 278,  94, 280,  94, 156, 281, 282, 174, 284, 285, 286])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY TARGET LABELS (LAST WORD OF EACH SEQUENCE)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7tYoK6hzQeP"
      },
      "source": [
        "## **CUSTOM DATASET CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fR059hVd-IAf"
      },
      "outputs": [],
      "source": [
        "# CUSTOM DATASET CLASS FOR PYTORCH DATALOADER\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        # INITIALIZE WITH INPUT FEATURES AND TARGET LABELS\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        # RETURN THE NUMBER OF SAMPLES IN DATASET\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # RETURN A SINGLE SAMPLE AT GIVEN INDEX\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ8K8503zQeP"
      },
      "source": [
        "## **CREATING DATASET INSTANCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KLX0clQM_j9r"
      },
      "outputs": [],
      "source": [
        "# CREATE DATASET INSTANCE WITH INPUT-OUTPUT PAIRS\n",
        "dataset = CustomDataset(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJcAtAdCzQeQ"
      },
      "source": [
        "## **DATASET SIZE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYHaeSuI_nJX",
        "outputId": "2d2f72bd-0982-40d2-9e7b-07d559dd1ee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "938"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DISPLAY THE SIZE OF THE DATASET\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " tensor(2))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FIRST DATASET\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21Ylb9QzQeR"
      },
      "source": [
        "## **CREATING DATALOADER FOR TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7ZUeD3l6_oZ3"
      },
      "outputs": [],
      "source": [
        "# CREATE DATALOADER FOR BATCH PROCESSING DURING TRAINING\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,   # PROCESS 32 SAMPLES AT A TIME\n",
        "    shuffle=True     # SHUFFLE DATA FOR BETTER TRAINING\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRAr2EZlzQeT"
      },
      "source": [
        "## **LSTM MODEL ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TEukXmWDEn8"
      },
      "outputs": [],
      "source": [
        "# LSTM NEURAL NETWORK MODEL FOR NEXT WORD PREDICTION\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # EMBEDDING LAYER TO CONVERT WORD INDICES TO DENSE VECTORS\n",
        "        self.embedding = nn.Embedding(vocab_size, 100)\n",
        "        \n",
        "        # LSTM LAYER FOR SEQUENCE PROCESSING\n",
        "        self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "        \n",
        "        # FULLY CONNECTED LAYER FOR FINAL PREDICTION\n",
        "        self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CONVERT WORD INDICES TO EMBEDDINGS\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        # PROCESS THROUGH LSTM\n",
        "        intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
        "        \n",
        "        # GENERATE FINAL PREDICTION\n",
        "        output = self.fc(final_hidden_state.squeeze(0))\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1czpt6TzQeT"
      },
      "source": [
        "## **MODEL INSTANTIATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YcQEVc9aVgr5"
      },
      "outputs": [],
      "source": [
        "# CREATE MODEL INSTANCE WITH VOCABULARY SIZE\n",
        "model = LSTMModel(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNBXmFfUzQeU"
      },
      "source": [
        "## **DEVICE CONFIGURATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Lvm7W6L1X6P1"
      },
      "outputs": [],
      "source": [
        "# SET DEVICE TO GPU IF AVAILABLE, OTHERWISE CPU\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSknLKC0zQeU"
      },
      "source": [
        "## **MOVING MODEL TO DEVICE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXwq43NRYD3q",
        "outputId": "89b61477-3c06-43f6-eb8f-ad3a8ad1f1a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(287, 100)\n",
              "  (lstm): LSTM(100, 150, batch_first=True)\n",
              "  (fc): Linear(in_features=150, out_features=287, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MOVE MODEL TO THE SELECTED DEVICE (GPU/CPU)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUvLRHHbzQeV"
      },
      "source": [
        "## **TRAINING CONFIGURATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1faORN1VYFdu"
      },
      "outputs": [],
      "source": [
        "# TRAINING HYPERPARAMETERS\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# LOSS FUNCTION FOR MULTI-CLASS CLASSIFICATION\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# OPTIMIZER FOR GRADIENT-BASED LEARNING\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BUNTnJWzQeV"
      },
      "source": [
        "## **MODEL TRAINING LOOP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRLc1cbrYVVV",
        "outputId": "50e9e431-5dfd-4c21-ffb3-841addd54db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 164.6494\n",
            "Epoch: 2, Loss: 146.6044\n",
            "Epoch: 3, Loss: 134.9143\n",
            "Epoch: 4, Loss: 123.8493\n",
            "Epoch: 5, Loss: 111.6659\n",
            "Epoch: 6, Loss: 100.7967\n",
            "Epoch: 7, Loss: 90.6241\n",
            "Epoch: 8, Loss: 80.8451\n",
            "Epoch: 9, Loss: 71.9084\n",
            "Epoch: 10, Loss: 63.4535\n",
            "Epoch: 11, Loss: 56.8092\n",
            "Epoch: 12, Loss: 49.3856\n",
            "Epoch: 13, Loss: 44.1279\n",
            "Epoch: 14, Loss: 38.4610\n",
            "Epoch: 15, Loss: 34.2062\n",
            "Epoch: 16, Loss: 29.7877\n",
            "Epoch: 17, Loss: 26.8238\n",
            "Epoch: 18, Loss: 23.4308\n",
            "Epoch: 19, Loss: 21.0255\n",
            "Epoch: 20, Loss: 18.5776\n",
            "Epoch: 21, Loss: 16.6975\n",
            "Epoch: 22, Loss: 15.3839\n",
            "Epoch: 23, Loss: 13.7523\n",
            "Epoch: 24, Loss: 12.6955\n",
            "Epoch: 25, Loss: 11.5811\n",
            "Epoch: 26, Loss: 10.9263\n",
            "Epoch: 27, Loss: 10.2216\n",
            "Epoch: 28, Loss: 9.4419\n",
            "Epoch: 29, Loss: 8.7265\n",
            "Epoch: 30, Loss: 8.2339\n",
            "Epoch: 31, Loss: 8.0680\n",
            "Epoch: 32, Loss: 7.6746\n",
            "Epoch: 33, Loss: 7.1535\n",
            "Epoch: 34, Loss: 6.8587\n",
            "Epoch: 35, Loss: 6.4891\n",
            "Epoch: 36, Loss: 6.3911\n",
            "Epoch: 37, Loss: 6.3022\n",
            "Epoch: 38, Loss: 5.8345\n",
            "Epoch: 39, Loss: 5.9171\n",
            "Epoch: 40, Loss: 5.5315\n",
            "Epoch: 41, Loss: 5.3520\n",
            "Epoch: 42, Loss: 5.1658\n",
            "Epoch: 43, Loss: 5.0342\n",
            "Epoch: 44, Loss: 4.9665\n",
            "Epoch: 45, Loss: 4.8685\n",
            "Epoch: 46, Loss: 4.9683\n",
            "Epoch: 47, Loss: 4.7099\n",
            "Epoch: 48, Loss: 4.7339\n",
            "Epoch: 49, Loss: 4.5883\n",
            "Epoch: 50, Loss: 4.4472\n"
          ]
        }
      ],
      "source": [
        "# MAIN TRAINING LOOP\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # PROCESS EACH BATCH\n",
        "    for batch_x, batch_y in dataloader:\n",
        "\n",
        "        # MOVE BATCH TO DEVICE\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # CLEAR GRADIENTS FROM PREVIOUS ITERATION\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # FORWARD PASS\n",
        "        output = model(batch_x)\n",
        "\n",
        "        # CALCULATE LOSS\n",
        "        loss = criterion(output, batch_y)\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        loss.backward()\n",
        "\n",
        "        # UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "\n",
        "        # ACCUMULATE TOTAL LOSS\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "    # PRINT EPOCH PROGRESS ONLY ONCE PER EPOCH\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKQsb_QYzQeX"
      },
      "source": [
        "## **PREDICTION FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "O9f6DkX-ZM-r"
      },
      "outputs": [],
      "source": [
        "# FUNCTION TO PREDICT NEXT WORD GIVEN INPUT TEXT\n",
        "def prediction(model, vocab, text):\n",
        "\n",
        "    # SET MODEL TO EVALUATION MODE\n",
        "    model.eval()\n",
        "\n",
        "    # TOKENIZE INPUT TEXT\n",
        "    tokenized_text = word_tokenize(text.lower())\n",
        "\n",
        "    # CONVERT TEXT TO NUMERICAL INDICES\n",
        "    numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "\n",
        "    # PAD THE SEQUENCE TO MATCH TRAINING LENGTH\n",
        "    # Get the sequence length from X (input features tensor)\n",
        "    seq_len = X.shape[1]\n",
        "    padded_text = torch.tensor([0] * (seq_len - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # MOVE TO DEVICE\n",
        "    padded_text = padded_text.to(device)\n",
        "\n",
        "    # DISABLE GRADIENT COMPUTATION FOR INFERENCE\n",
        "    with torch.no_grad():\n",
        "        # GET MODEL PREDICTION\n",
        "        output = model(padded_text)\n",
        "\n",
        "        # GET INDEX OF MOST LIKELY NEXT WORD\n",
        "        _, index = torch.max(output, dim=1)\n",
        "\n",
        "    # RETURN TEXT WITH PREDICTED NEXT WORD\n",
        "    return text + \" \" + list(vocab.keys())[index.item()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-tulNykzQeX"
      },
      "source": [
        "## **SINGLE PREDICTION TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VsRgcJysbGCg",
        "outputId": "18d67b6f-4a7f-4aaa-cd23-7886df15785f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The course follows a monthly subscription'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TEST SINGLE PREDICTION WITH SAMPLE TEXT\n",
        "prediction(model, vocab, \"The course follows a monthly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Wc43ELzQeY"
      },
      "source": [
        "## **MULTI-STEP PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JPACfEbNPo",
        "outputId": "5673c2b8-2902-4625-b200-315a35699572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting text: You will have\n",
            "--------------------------------------------------\n",
            "Step 1: You will have to\n",
            "Step 2: You will have to fill\n",
            "Step 3: You will have to fill a\n",
            "Step 4: You will have to fill a google\n",
            "Step 5: You will have to fill a google form\n",
            "Step 6: You will have to fill a google form provided\n",
            "Step 7: You will have to fill a google form provided in\n",
            "Step 8: You will have to fill a google form provided in your\n",
            "Step 9: You will have to fill a google form provided in your dashboard\n",
            "Step 10: You will have to fill a google form provided in your dashboard and\n",
            "\n",
            "Prediction sequence completed!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# GENERATE MULTIPLE CONSECUTIVE PREDICTIONS\n",
        "num_tokens = 10\n",
        "input_text = \"You will have\"\n",
        "\n",
        "print(f\"Starting text: {input_text}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ITERATIVELY PREDICT NEXT WORDS\n",
        "for i in range(num_tokens):\n",
        "    # GET PREDICTION\n",
        "    output_text = prediction(model, vocab, input_text)\n",
        "\n",
        "    # PRINT ONLY THE NEW PREDICTION WITH CLEAR FORMATTING\n",
        "    print(f\"Step {i+1}: {output_text}\")\n",
        "\n",
        "    # UPDATE INPUT FOR NEXT ITERATION\n",
        "    input_text = output_text\n",
        "\n",
        "    # SMALL DELAY FOR READABILITY\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"\\nPrediction sequence completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf2SQksyzQeY"
      },
      "source": [
        "## **CREATING EVALUATION DATALOADER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JXsV4AnNXNnw"
      },
      "outputs": [],
      "source": [
        "# CREATE DATALOADER WITHOUT SHUFFLING FOR EVALUATION\n",
        "dataloader1 = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False  # NO SHUFFLING FOR CONSISTENT EVALUATION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8devDt0zQeZ"
      },
      "source": [
        "## **MODEL ACCURACY CALCULATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py7o0rJJc5pm",
        "outputId": "8c9fce6a-7a92-4ff8-d46d-e057c666a120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 95.63%\n"
          ]
        }
      ],
      "source": [
        "# FUNCTION TO CALCULATE MODEL ACCURACY ON DATASET\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "\n",
        "    # SET MODEL TO EVALUATION MODE (DISABLES DROPOUT, ETC.)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # DISABLE GRADIENT COMPUTATION FOR EVALUATION\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:  # FIXED: USE PARAMETER INSTEAD OF GLOBAL VARIABLE\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # GET MODEL PREDICTIONS\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            # GET PREDICTED WORD INDICES\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            # COMPARE PREDICTIONS WITH ACTUAL LABELS\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    # CALCULATE ACCURACY PERCENTAGE\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy\n",
        "\n",
        "# COMPUTE AND DISPLAY MODEL ACCURACY\n",
        "accuracy = calculate_accuracy(model, dataloader, device)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
